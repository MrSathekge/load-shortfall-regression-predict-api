#!/usr/bin/env python
# coding: utf-8

# # Regression Predict Student Solution
# 
# © Explore Data Science Academy
# 
# ---
# ### Honour Code
# 
# I {**YOUR NAME, YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).
# 
# Non-compliance with the honour code constitutes a material breach of contract.
# 
# ### Predict Overview: Spain Electricity Shortfall Challenge
# 
# The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:
# 
# - 1. analyse the supplied data;
# - 2. identify potential errors in the data and clean the existing data set;
# - 3. determine if additional features can be added to enrich the data set;
# - 4. build a model that is capable of forecasting the three hourly demand shortfalls;
# - 5. evaluate the accuracy of the best machine learning model;
# - 6. determine what features were most important in the model’s prediction decision, and
# - 7. explain the inner working of the model to a non-technical audience.
# 
# Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:
# 
# > In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.
#  
# On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. 

# <a id="cont"></a>
# 
# ## Table of Contents
# 
# <a href=#one>1. Importing Packages</a>
# 
# <a href=#two>2. Loading Data</a>
# 
# <a href=#three>3. Exploratory Data Analysis (EDA)</a>
# 
# <a href=#four>4. Data Engineering</a>
# 
# <a href=#five>5. Modeling</a>
# 
# <a href=#six>6. Model Performance</a>
# 
# <a href=#seven>7. Model Explanations</a>

#  <a id="one"></a>
# ## 1. Importing Packages
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Importing Packages ⚡ |
# | :--------------------------- |
# | In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |
# 
# ---

# In[1]:


# Importing of libraries for loading data, performing data manipulation and data visulisation

import numpy as np # Linear algebra
import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # Visualization packages used to plot graphs
get_ipython().run_line_magic('matplotlib', 'inline')
from scipy import stats # Package for statistics
import warnings
warnings.filterwarnings('ignore')

# Libraries for data preparation and model building
import seaborn as sns
sns.set(color_codes=True)
from sklearn.metrics import mean_squared_error
from sklearn import *
from sklearn.linear_model import *

# Setting global constants to ensure notebook results are reproducible
PARAMETER_CONSTANT =np.random.seed(1)


# In[119]:


# Already imported pandas as pd (The following will enable us to see the entire data set when code is being run.)
# Refer: 
# https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)


# <a id="two"></a>
# ## 2. Loading the Data
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Loading the data ⚡ |
# | :--------------------------- |
# | In this section you are required to load the data from the `df_train` file into a DataFrame. |
# 
# ---

# In[2]:


# Load the data csv files
# NB: All files are placed in same folder as this notebook
#df_sample = pd.read_csv('sample_submission_load_shortfall.csv')   # downloaded this from Kaggle competition

df_train = pd.read_csv('df_train.csv')
df_test = pd.read_csv('df_test.csv')


# <a id="three"></a>
# ## 3. Exploratory Data Analysis (EDA)
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Exploratory data analysis ⚡ |
# | :--------------------------- |
# | In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |
# 
# ---
# 

# In[121]:


# At this stage we begin with a brief overview of the data 
# Through this initial process we aim to get a basic understanding of the data set


# In[122]:


# Begin by viewing the first few colums and rows
df_train.head(10)


# In[123]:


# Viewing the amount of rows and columns in the data set
df_train.shape


# In[124]:


# Here we are taking a look at the data types to check if there aren't any discrepancies
df_train.info()


# In[125]:


# Applying the same method to our test data(It was found that the test data set has the same type of layout. Hence it 
# was not necessary to display the entire introductory process with the test data)

df_test.head()


# In[126]:


# Taking a look at the statistics
df_train.describe()


# In[127]:


# Viewing the possible outliers within the data
df_train.kurtosis()


# In[128]:


# Grouping the most extreme of outliers that fall within a 3 hour period. 

df_train[['Barcelona_rain_1h','Seville_rain_1h','Bilbao_snow_3h','Barcelona_pressure','Seville_rain_3h','Madrid_rain_1h','Barcelona_rain_3h','Valencia_snow_3h']].head(5)


# In[129]:


#Visualizing the aforementioned outliers

fig, axes = plt.subplots(3, 3, figsize=(15, 15))
  
sns.boxplot(ax=axes[0, 0], data=df_train, x='Barcelona_rain_1h')
sns.boxplot(ax=axes[0, 1], data=df_train, x='Seville_rain_1h')
sns.boxplot(ax=axes[0, 2], data=df_train, x='Bilbao_snow_3h')
sns.boxplot(ax=axes[1, 0], data=df_train, x='Barcelona_pressure')
sns.boxplot(ax=axes[1, 1], data=df_train, x='Seville_rain_3h')
sns.boxplot(ax=axes[1, 2], data=df_train, x='Madrid_rain_1h')
sns.boxplot(ax=axes[2, 0], data=df_train, x='Barcelona_rain_3h')
sns.boxplot(ax=axes[2, 1], data=df_train, x='Bilbao_wind_speed')
sns.boxplot(ax=axes[2, 2], data=df_train, x='Valencia_snow_3h')


# In[130]:


# Viwing the correlation predictor variables
# (To help theorize if there could be any connection between our response variable 'load shortfall_3h')
# (This was also done to help us decide whether they were worth keep or removing before building our model)

df_train.corr()


# In[131]:


# A graphical representation of the correlated values
# Those in a dark red signify a strong positive correlation while the converse is true for those that are very light

plt.figure(figsize = (25,30))
plt.title('features correlation plot')
corr = df_train.corr()
sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,annot = True,linewidths=.1,cmap="Reds")
plt.show()


# In[132]:


## A orrelation matrix function is created to isolate the highly correlated features

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr


# In[133]:


# A count of the highly correlated features
corr_features = correlation(df_train, 0.8)
len(set(corr_features))


# In[134]:


# A display of the the highly correlated features
corr_features = correlation(df_train, 0.8)
corr_features


# In[135]:


correlated_features = df_train[['Barcelona_temp','Barcelona_temp_max','Barcelona_temp_min','Bilbao_temp','Bilbao_temp_max','Bilbao_temp_min','Madrid_temp','Madrid_temp_max','Madrid_temp_min','Seville_temp','Seville_temp_min','Valencia_temp','Valencia_temp_min']]
correlated_features.head(5)


# In[136]:


# A graphical representation of only the ighly correlated values
plt.figure(figsize = (10,10))
plt.title('features correlation plot')
corr = correlated_features.corr()
sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,annot = True,linewidths=.1,cmap="Reds")
plt.show()


# In[137]:


# Further graphs to help visualitze features the highly correlated values
fig, axes = plt.subplots(2, 3, figsize=(10, 10))
sns.scatterplot(ax=axes[0, 0], x='Bilbao_temp',y='Barcelona_temp', data=correlated_features)
sns.scatterplot(ax=axes[0, 1], x='Madrid_temp',y='Seville_temp', data=correlated_features)
sns.scatterplot(ax=axes[0, 2], x='Seville_temp',y='Barcelona_temp', data=correlated_features)
sns.scatterplot(ax=axes[1, 0], x='Valencia_temp',y='Madrid_temp', data=correlated_features)
sns.scatterplot(ax=axes[1, 1], x='Barcelona_temp',y='Madrid_temp', data=correlated_features)
sns.scatterplot(ax=axes[1, 2], x='Valencia_temp',y='Seville_temp', data=correlated_features)


# In[138]:


# Vizualization of skewness of the correlated_features data in order to see whether ouliers are positively or negatively skewed
# This will help in conceptualizing the affect of outliers on the data
correlated_features.skew()


# In[139]:


# Plot relevant feature interactions
average_temp = df_train[['Barcelona_temp','Bilbao_temp','Madrid_temp','Seville_temp','Valencia_temp']]

average_temp.plot(kind='density', subplots=True, layout=(3, 3), sharex=False, figsize=(15, 15));


# ***TESTING NORMALITY AND EQUALITY OF VARIANCE***

# Testing if the Dataset is normaly distributed through a graphical representation of a `histogram` and a `Q-Q plot`
# and finally perform a Formal Statistical Test using a `Shapiro-Wilk Test`. 

# **Graphical Normality Test**

# In[140]:


sns.distplot(a=df_train['load_shortfall_3h'], color='blue',hist_kws={"edgecolor": 'white'})
plt.show()


# The  histogram shows, that the `load_shortfall_3h` exhibits a **bell-shape** and is `normally distributed`, 
# showing that the majority of the loadshorfall values are clustered about the mean and less at the extremes.
# 

# In[141]:


# Now we take a look at the QQplot

stats.probplot(df_train['load_shortfall_3h'], dist="norm", plot=plt)
plt.show()


# the values follow the theoretical quantiles and thus lie along the 45-degree diagonal which means that the datase is Normally Distributed

# **Shapiro-Wilk Test**
# for performing a normality formal test the Hypothesis would be:
# 
# $$H_0: \mu = \mu2 =...= \mu$$  $$VS$$
# $$H_1: \mu \ne \mu$$
# the test is done under a **significance level** of 5% and would then **reject** the **null Hypothesis** if
# $p$-value < 5%

# In[142]:


#perform Shapiro-Wilk test for normality
from scipy.stats import shapiro 
shapiro(df_train['load_shortfall_3h'])


# Since the $p$-value=1.6289 is greater than 0.05, we accept the null hypothesis of the Shapiro-Wilk test.
# With all the three Phases of Normal Testing met This means we have sufficient evidence to say that the load_shortfall_3h data comes from a normal distribution.

# # Using Pandas Profiling to create a EDA Report
# ### Remove the #'s below and run the cell to see the report

# In[143]:


#from dataprep.eda import create_report

#df = pd.read_csv("df_train.csv")
#create_report(df)


# <a id="four"></a>
# ## 4. Data Engineering
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Data engineering ⚡ |
# | :--------------------------- |
# | In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |
# 
# ---

# In[144]:


# Remove missing values/ features


# In[3]:


df_train.isnull().sum()


# In[4]:


# Do the same for the test data
df_test.isnull().sum()


# ### After dropping the null values they are to be imputed either the mean, mode or median
# ### The mode was selected on the basis that wind pressure tends to be the same

# In[5]:


df_train['Valencia_pressure'].mode()


# In[6]:


df_train['Valencia_pressure'].mean()


# In[7]:


df_train['Valencia_pressure'].median()


# ### Copying of datasets, to continuing working on copies - in case  mistakes are made.

# In[8]:


df_train_copy = df_train.copy()
df_test_copy = df_test.copy()


# ###  Filling of nulls in both data sets (train and test) with the mode

# In[9]:


df_train_copy['Valencia_pressure'] = df_train_copy['Valencia_pressure'].fillna(df_train_copy['Valencia_pressure'].mode()[0])


# In[10]:


# Check
df_train_copy.isnull().sum()


# In[11]:


df_test_copy['Valencia_pressure'] = df_test_copy['Valencia_pressure'].fillna(df_test_copy['Valencia_pressure'].mode()[0])


# In[12]:


# Check
df_test_copy.isnull().sum()


# ### Checking data types that need changing

# In[155]:


df_train_copy.dtypes


# In[156]:


df_test_copy.dtypes


# ### The time column datatype needs to be changed. So it can be used in the processing -(Applied to both copied data sets)

# In[157]:


df_train_copy['time']   # to see what it looks like


# In[13]:


# Converting type to datetime int format 
df_train_copy['time'] = pd.to_datetime(df_train_copy['time'])


# In[14]:


# Converting type to datetime int format 

df_test_copy['time'] = pd.to_datetime(df_test_copy['time'])


# In[160]:


# Checking changes made to datatype

df_train_copy['time']


# ### To help optimize the model the time column is split into year, month, day, hour and weekday.

# In[15]:


df_train_copy['Year'] = df_train_copy['time'].dt.year
df_train_copy['Month'] = df_train_copy['time'].dt.month
df_train_copy['Weekday'] = df_train_copy['time'].dt.weekday
df_train_copy['Day'] = df_train_copy['time'].dt.day
df_train_copy['Hour'] = df_train_copy['time'].dt.hour


# In[16]:


df_test_copy['Year'] = df_test_copy['time'].dt.year
df_test_copy['Month'] = df_test_copy['time'].dt.month
df_test_copy['Weekday'] = df_test_copy['time'].dt.weekday
df_test_copy['Day'] = df_test_copy['time'].dt.day
df_test_copy['Hour'] = df_test_copy['time'].dt.hour


# In[17]:


df_train_copy.head()


# In[18]:


df_test_copy.head()


# ### Dropping original Time column (When reproducing the model the original time values cannot be used)

# In[165]:


df_train_copy.drop('time', axis = 1, inplace = True)


# In[166]:


df_test_copy.drop('time', axis = 1, inplace = True)


# In[167]:


df_train_copy.head()


# ### Dropping the string preceding Valencia pressure number - on both data sets
# - Extracting the numbers, and changing the data type from object to numeric
# - WHY? Models can only work with numerical values
# -After changing to numeric we will randomise the numbers instead of having 1 till 10 only

# In[168]:


df_train_copy['Valencia_wind_deg']


# In[169]:


df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].str.extract('(\d+)')
#                                                                       # this tells it to look for a number
# This is a Regular Expression pattern \d is a regex pattern for digit + is a regex pattern for at least 
# (one or more) since they are enclosed in a ( ) that means the group that you want to capture.
# https://community.dataquest.io/t/str-extract-r-d-what-does-d-mean/506096/2


# In[170]:


df_train_copy['Valencia_wind_deg']


# In[171]:


df_train_copy['Valencia_wind_deg'] = pd.to_numeric(df_train_copy['Valencia_wind_deg'])
# to datetime, to_numeric, to_float, to_string

# we dont need to code our own function to convert it, we can just use pandas


# In[172]:


#Evaluating same weather conditions by cities

df_train_copy[['Valencia_wind_deg', 'Bilbao_wind_deg', 'Barcelona_wind_deg', 'load_shortfall_3h']].head(20)


# In[173]:


# Using other cities wind deg to estimate the random values on valencia

df_train_copy['Bilbao_wind_deg'].max()


# In[174]:


df_train_copy['Barcelona_wind_deg'].max()


# In[175]:


import random
        
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(1.0, random.uniform(0,36))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(2.0, random.uniform(37,72))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(3.0, random.uniform(73,108))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(4.0, random.uniform(109,144))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(5.0, random.uniform(145,180))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(6.0, random.uniform(181,216))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(7.0, random.uniform(217,252))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(8.0, random.uniform(253,288))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(9.0, random.uniform(289,324))
df_train_copy['Valencia_wind_deg'] = df_train_copy['Valencia_wind_deg'].replace(10.0, random.uniform(325,360))


# In[176]:


df_clean = df_train_copy
df_clean['Valencia_pressure'] = df_clean['Valencia_pressure'].fillna(df_clean['Valencia_pressure'].mode()[0])


# In[177]:


df_clean['Valencia_wind_deg']


# In[178]:


# Check the valencia wind degree data makes more sense

df_train_copy[['Valencia_wind_deg', 'Bilbao_wind_deg', 'Barcelona_wind_deg', 'load_shortfall_3h']].head(20)


# Now we will do the same with the test copy data, the valencia wind degree data set

# In[179]:


df_test_copy['Valencia_wind_deg']


# In[180]:


df_test_copy['Valencia_wind_deg']


# In[181]:


df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].str.extract('(\d+)')


# In[182]:


df_test_copy['Valencia_wind_deg'] = pd.to_numeric(df_test_copy['Valencia_wind_deg'])


# In[183]:


#Evaluating same weather conditions by cities

df_test_copy[['Valencia_wind_deg', 'Bilbao_wind_deg', 'Barcelona_wind_deg']].head(20)


# In[184]:


df_test_copy['Bilbao_wind_deg'].max()


# In[185]:


df_test_copy['Barcelona_wind_deg'].max()


# We used the values 1 till 10 and created intervals according to the other cities intervals

# In[186]:


import random
        
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(1.0, random.uniform(0,36))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(2.0, random.uniform(37,72))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(3.0, random.uniform(73,108))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(4.0, random.uniform(109,144))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(5.0, random.uniform(145,180))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(6.0, random.uniform(181,216))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(7.0, random.uniform(217,252))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(8.0, random.uniform(253,288))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(9.0, random.uniform(289,324))
df_test_copy['Valencia_wind_deg'] = df_test_copy['Valencia_wind_deg'].replace(10.0, random.uniform(325,360))


# In[187]:


df_test_copy['Valencia_wind_deg']


# In[188]:


df_test_copy[['Valencia_wind_deg', 'Bilbao_wind_deg', 'Barcelona_wind_deg']].head(20)


# ## ### Dropping the string preceding the Seville pressure column - on both data sets
# - Extracting the numbers, and changing the data type

# In[189]:


df_train_copy['Seville_pressure']


# In[190]:


df_train_copy['Seville_pressure'] = df_train_copy['Seville_pressure'].str.extract('(\d+)')


# In[191]:


df_train_copy['Seville_pressure']


# In[192]:


df_train_copy['Seville_pressure'] = pd.to_numeric(df_train_copy['Seville_pressure'])


# In[193]:


df_train_copy['Seville_pressure'] # To confirm data type change


# In[194]:


df_test_copy['Seville_pressure'] = df_test_copy['Seville_pressure'].str.extract('(\d+)')


# In[195]:


df_test_copy['Seville_pressure']


# In[196]:


df_test_copy['Seville_pressure'] = pd.to_numeric(df_test_copy['Seville_pressure'])


# In[197]:


df_test_copy['Seville_pressure'] # To confirm data type change


# ### Dropping of any unnecessary columns gthat won't aid the model building

# In[198]:


df_train_copy = df_train_copy.drop(['Unnamed: 0'], axis = 1) # axis =1 tells it to drop the column, vertically


# In[199]:


df_test_copy = df_test_copy.drop(['Unnamed: 0'], axis = 1) # axis =1 tells it to drop the column, vertically


# ### Moving load_shortfall_3h to last column (for ease of interpretation)

# In[200]:


df_train_copy.columns


# In[201]:


df_train_copy = df_train_copy[['Madrid_wind_speed', 'Valencia_wind_deg', 'Bilbao_rain_1h', 'Valencia_wind_speed', 'Seville_humidity', 'Madrid_humidity', 'Bilbao_clouds_all', 'Bilbao_wind_speed', 'Seville_clouds_all', 'Bilbao_wind_deg', 'Barcelona_wind_speed', 'Barcelona_wind_deg', 'Madrid_clouds_all', 'Seville_wind_speed', 'Barcelona_rain_1h', 'Seville_pressure', 'Seville_rain_1h', 'Bilbao_snow_3h', 'Barcelona_pressure', 'Seville_rain_3h', 'Madrid_rain_1h', 'Barcelona_rain_3h', 'Valencia_snow_3h', 'Madrid_weather_id', 'Barcelona_weather_id', 'Bilbao_pressure', 'Seville_weather_id', 'Valencia_pressure', 'Seville_temp_max', 'Madrid_pressure', 'Valencia_temp_max', 'Valencia_temp', 'Bilbao_weather_id', 'Seville_temp', 'Valencia_humidity', 'Valencia_temp_min', 'Barcelona_temp_max', 'Madrid_temp_max', 'Barcelona_temp', 'Bilbao_temp_min', 'Bilbao_temp', 'Barcelona_temp_min', 'Bilbao_temp_max', 'Seville_temp_min', 'Madrid_temp', 'Madrid_temp_min','Year', 'Month', 'Day', 'Hour', 'Weekday', 'load_shortfall_3h']]


# In[202]:


df_test_copy = df_test_copy[['Madrid_wind_speed', 'Valencia_wind_deg', 'Bilbao_rain_1h', 'Valencia_wind_speed', 'Seville_humidity', 'Madrid_humidity', 'Bilbao_clouds_all', 'Bilbao_wind_speed', 'Seville_clouds_all', 'Bilbao_wind_deg', 'Barcelona_wind_speed', 'Barcelona_wind_deg', 'Madrid_clouds_all', 'Seville_wind_speed', 'Barcelona_rain_1h', 'Seville_pressure', 'Seville_rain_1h', 'Bilbao_snow_3h', 'Barcelona_pressure', 'Seville_rain_3h', 'Madrid_rain_1h', 'Barcelona_rain_3h', 'Valencia_snow_3h', 'Madrid_weather_id', 'Barcelona_weather_id', 'Bilbao_pressure', 'Seville_weather_id', 'Valencia_pressure', 'Seville_temp_max', 'Madrid_pressure', 'Valencia_temp_max', 'Valencia_temp', 'Bilbao_weather_id', 'Seville_temp', 'Valencia_humidity', 'Valencia_temp_min', 'Barcelona_temp_max', 'Madrid_temp_max', 'Barcelona_temp', 'Bilbao_temp_min', 'Bilbao_temp', 'Barcelona_temp_min', 'Bilbao_temp_max', 'Seville_temp_min', 'Madrid_temp', 'Madrid_temp_min','Year', 'Month', 'Day', 'Hour', 'Weekday']]


# In[203]:


df_train_copy.head(2)


# <a id="five"></a>
# ## 5. Modelling
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Modelling ⚡ |
# | :--------------------------- |
# | In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |
# 
# ---

# # Model 1: Linear Regression

# In[204]:


X = df_train_copy.drop('load_shortfall_3h', axis = 1)
y = df_train_copy['load_shortfall_3h']


# In[205]:


from sklearn.model_selection import train_test_split
# split data into training and validation data, for both features and target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)


# In[206]:


from sklearn.linear_model import LinearRegression


# In[207]:


# Declare the model object
lm_uncleaned = LinearRegression()


# In[208]:


lm_uncleaned.fit(X_train, y_train)


# In[209]:


prediction_uncleaned = lm_uncleaned.predict(X_train)


# In[210]:


from sklearn import metrics


# In[211]:


prediction_uncleaned_y_test = lm_uncleaned.predict(X_test)


# In[212]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction_uncleaned_y_test)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, prediction_uncleaned_y_test))


# # Model 2 : Decision Tree

# In[213]:


from sklearn.tree import DecisionTreeRegressor


# In[214]:


reg_tree = DecisionTreeRegressor(max_depth=3)


# In[215]:


reg_tree.fit(X_train, y_train)


# In[216]:


y_pred_ds = reg_tree.predict(X_test)


# In[217]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_ds)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, y_pred_ds))


# # Model 3 : Random Forest

# In[218]:


from sklearn.ensemble import RandomForestRegressor


# In[219]:


# Our forest has 100 trees, max depth of 5 
rf = RandomForestRegressor(n_estimators=100, max_depth=5)


# In[220]:


rf.fit(X_train,y_train)


# In[221]:


y_pred_rf = rf.predict(X_test)


# In[222]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, y_pred_rf))


# # Model 4 : Adaboost regressor

# In[223]:


from sklearn.ensemble import AdaBoostRegressor


# In[224]:


# Instantiate BaggingRegressor model with a decision tree as the bas model
ag_reg = AdaBoostRegressor(base_estimator=lm_uncleaned)


# In[225]:


ag_reg.fit(X_train,y_train)


# In[226]:


y_pred_ag = ag_reg.predict(X_test)


# In[227]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_ag)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, y_pred_ag))


# # Model 5 : XGBoost

# In[229]:


pip install xgboost


# In[230]:


import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score


# In[231]:


X, y = df_train_copy.iloc[:,:-1], df_train_copy.iloc[:,-1]


# In[232]:


from sklearn.model_selection import train_test_split


# In[233]:


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=123)


# In[234]:


xg_reg = xgb.XGBRegressor()


# In[235]:


xg_reg.fit(X_train, y_train)


# In[236]:


preds_xg = xg_reg.predict(X_test)


# In[237]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, preds_xg)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, preds_xg))


# # Model 6 : Stacking Regressor

# In[238]:


from sklearn.ensemble import StackingRegressor


# In[239]:


models = [('LR', lm_uncleaned), ('XGB', xg_reg), ('ADA', ag_reg)]


# In[240]:


meta_learner_reg = LinearRegression()


# In[241]:


s_reg = StackingRegressor(estimators=models, final_estimator=meta_learner_reg)


# In[242]:


s_reg.fit(X_train, y_train)


# In[243]:


s_reg.fit(X_train, y_train)


# In[244]:


y_pred_s = s_reg.predict(X_test)


# In[245]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_s)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, y_pred_s))


# # Model 7 : Voting Regressor

# In[246]:


from sklearn.ensemble import VotingRegressor


# In[247]:


models = [('ADA', ag_reg), ('XGB', xg_reg), ('RF', rf)]


# In[248]:


model_weightings = np.array([0.1, 0.6, 0.3])
# 3560 kaggle: score 10% lin regr, 60% xgboost, 30% rf
# 10% lin regr, 70% xgboost, 20% rf


# In[249]:


v_reg = VotingRegressor(estimators=models, weights=model_weightings)


# In[250]:


v_reg.fit(X_train, y_train)


# In[251]:


y_pred_v = v_reg.predict(X_test)


# In[252]:


#Calculate RMSE metric
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_v)))

# Calculate the R-squared metric
print('R_squared:', metrics.r2_score(y_test, y_pred_v))


# <a id="six"></a>
# ## 6. Model Performance
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Model performance ⚡ |
# | :--------------------------- |
# | In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |
# 
# ---

# In[ ]:


# Metrics results dictionary
results_dict = {'Training RMSE':
                    {
                        "(1) Linear Regression": np.sqrt(metrics.mean_squared_error( y_train, lm_uncleaned.predict(X_train) )),
                        "(2) Decision Tree": np.sqrt(metrics.mean_squared_error( y_train, reg_tree.predict(X_train) )),
                        "(3) Random Forest": np.sqrt(metrics.mean_squared_error( y_train, rf.predict(X_train) )),
                        "(4) Adaboost Regressor": np.sqrt(metrics.mean_squared_error( y_train, ag_reg.predict(X_train) )),
                        "(5) XGBoost": np.sqrt(metrics.mean_squared_error( y_train, xg_reg.predict( X_train))),
                        "(6) Voting Regressor": np.sqrt(metrics.mean_squared_error( y_train, v_reg.predict( X_train))),
                        "(7) Stacking Regressor": np.sqrt(metrics.mean_squared_error( y_train, s_reg.predict( X_train))) 
                        
                    },
                    'Test RMSE':
                    {
                        "(1) Linear Regression": np.sqrt(metrics.mean_squared_error(y_test, lm_uncleaned.predict(X_test))),
                        "(2) Decision Tree": np.sqrt(metrics.mean_squared_error(y_test, reg_tree.predict(X_test))),
                        "(3) Random Forest": np.sqrt(metrics.mean_squared_error(y_test, rf.predict(X_test))),
                        "(4) Adaboost Regressor": np.sqrt(metrics.mean_squared_error(y_test, ag_reg.predict(X_test))),
                        "(5) XGBoost": np.sqrt(metrics.mean_squared_error( y_test, xg_reg.predict(X_test) )),
                        "(6) Voting Regressor": np.sqrt(metrics.mean_squared_error( y_test, v_reg.predict(X_test) )),
                        "(7) Stacking Regressor": np.sqrt(metrics.mean_squared_error( y_test, s_reg.predict(X_test) ))
                    }
                }


# In[ ]:


# Create dataframe from dictionary
results_df = pd.DataFrame(data=results_dict)

results_df


# # BEST MODEL : Voting Regressor
# - Although XGBoost and Stacking regressor gave the lowest TEST RMSE, the model that performed best on Kaggle was the Voting Regressor.
# - We think this might be because the XGBoost and the Stacking Regressor might have overfit a little on the training data.
# ####  RMSE: 2874, r squared =0.68  ... xgboost :60%, rf:30%, ada: 10%... 
# #### Kaggle score: 3560  (#14)
# 

# In[ ]:


# Create figure and axes
f, ax = plt.subplots(figsize=(15,5), nrows=1, ncols=5, sharey=True)

# Create list of titles and predictions to use in for loop
pred = [lm_uncleaned.predict(X_test), reg_tree.predict(X_test), rf.predict(X_test), xg_reg.predict(X_test), v_reg.predict(X_test)]
title = ['Linear Regression','Decision tree', 'Random Forest', 'XGBoost', 'Voting Regressor']

# Loop through all axes to plot each model's results 
for i in range(5):
    rmse = round(np.sqrt(mean_squared_error(pred[i],y_test)))
    ax[i].set_title(title[i]+"  (RMSE: "+str(rmse)+ ")")
    ax[i].set_xlabel('Actual')
    ax[i].set_ylabel('Predicted')
    ax[i].plot(y_test,y_test,'r')
    ax[i].scatter(y_test,pred[i])


# In[ ]:


# convert prediction to dataframe
preds = v_reg.predict(df_test_copy)
new_daf = pd.DataFrame(preds, columns=['load_shortfall_3h'])
new_daf.head(5)


# In[ ]:


output = pd.DataFrame({'time':df_test['time']})
submission2 = output.join(new_daf)
submission2.to_csv('v_reg_time_split_weekday_hour_with_s_reg2.csv', index=False)


# # Saving the Voting Regressor model

# In[ ]:


import pickle

model_save_path = "voting_regressor_model_3560.pkl"
with open(model_save_path,'wb') as file:
    pickle.dump(v_reg,file)


# <a id="seven"></a>
# ## 7. Model Explanations
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Model explanation ⚡ |
# | :--------------------------- |
# | In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |
# 
# ---

# ## Best Performing Model : Voting Regressor
# - It is a type on heterogeneous ensemble model that combined different models together.
# - It then takes as input, different types of models.
# - We specify the type of models that we want to input in the voting regressor.
# - We then give set weights for those individual models.

# 
